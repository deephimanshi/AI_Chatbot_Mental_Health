{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3964dea",
   "metadata": {},
   "source": [
    "# üí¨ AI Chatbot for Mental Health Support\n",
    "---\n",
    "This project demonstrates a simple AI-powered chatbot that provides empathetic, supportive responses using Natural Language Processing (NLP)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc20d261",
   "metadata": {},
   "source": [
    "## üì¶ Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a36d780",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip install transformers torch streamlit nltk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916fbcbb",
   "metadata": {},
   "source": [
    "## üß† Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69b3844",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import nltk\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d102d31",
   "metadata": {},
   "source": [
    "## ü§ñ Load Pre-trained Conversational Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f5f64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load DialoGPT-small from Hugging Face\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-small\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-small\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c828808e",
   "metadata": {},
   "source": [
    "## üí¨ Define Chat Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d7fc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_input(text):\n",
    "    negative_words = [\"sad\", \"depressed\", \"angry\", \"anxious\", \"hopeless\", \"lonely\"]\n",
    "    if any(word in text.lower() for word in negative_words):\n",
    "        return text + \" Please respond with empathy and kindness.\"\n",
    "    return text\n",
    "\n",
    "def chat_with_bot():\n",
    "    print(\"Chatbot: Hello, I'm here to listen. How are you feeling today?\")\n",
    "    chat_history_ids = None\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() in [\"exit\", \"quit\", \"bye\"]:\n",
    "            print(\"Chatbot: Take care. You are doing your best üíõ\")\n",
    "            break\n",
    "        new_input_ids = tokenizer.encode(preprocess_input(user_input) + tokenizer.eos_token, return_tensors='pt')\n",
    "        bot_input_ids = torch.cat([chat_history_ids, new_input_ids], dim=-1) if chat_history_ids is not None else new_input_ids\n",
    "        chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\n",
    "        response = tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)\n",
    "        print(f\"Chatbot: {response}\")\n",
    "        \n",
    "# Run this function to start the chatbot\n",
    "# chat_with_bot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7441d5e",
   "metadata": {},
   "source": [
    "## üåê Optional: Create Streamlit Web App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b366b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save this code in a separate file (app.py) to run with: streamlit run app.py\n",
    "\n",
    "import streamlit as st\n",
    "\n",
    "st.title(\"üí¨ AI Mental Health Support Chatbot\")\n",
    "st.write(\"I‚Äôm here to listen and chat with you in a safe, supportive space.\")\n",
    "\n",
    "if \"history\" not in st.session_state:\n",
    "    st.session_state[\"history\"] = None\n",
    "\n",
    "user_input = st.text_input(\"You:\", \"\")\n",
    "\n",
    "if st.button(\"Send\") and user_input:\n",
    "    new_input_ids = tokenizer.encode(preprocess_input(user_input) + tokenizer.eos_token, return_tensors='pt')\n",
    "    bot_input_ids = torch.cat([st.session_state[\"history\"], new_input_ids], dim=-1) if st.session_state[\"history\"] is not None else new_input_ids\n",
    "    st.session_state[\"history\"] = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\n",
    "    response = tokenizer.decode(st.session_state[\"history\"][:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)\n",
    "    st.text_area(\"Chatbot:\", value=response, height=150)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
